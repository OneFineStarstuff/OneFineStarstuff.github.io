{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP6x+LjGFpdmRf+qQwoCRrX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff.github.io/blob/main/agi_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import asyncio\n",
        "from typing import List\n",
        "from PIL import Image\n",
        "from fastapi import FastAPI, UploadFile, Depends, HTTPException\n",
        "from fastapi.security import OAuth2PasswordBearer\n",
        "from pydantic import BaseModel\n",
        "import jwt\n",
        "import pyttsx3\n",
        "from loguru import logger\n",
        "import io\n",
        "import uvicorn\n",
        "import signal\n",
        "import sys\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from ultralytics import YOLO\n",
        "import whisper\n",
        "\n",
        "# === Load Configuration ===\n",
        "with open('config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# === Configuration and Logging Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.add(config[\"logging\"][\"log_file\"], rotation=config[\"logging\"][\"rotation\"], level=config[\"logging\"][\"level\"], enqueue=config[\"logging\"][\"enqueue\"], backtrace=config[\"logging\"][\"backtrace\"], diagnose=config[\"logging\"][\"diagnose\"])\n",
        "logger.info(\"Application startup\")\n",
        "\n",
        "# === Security Setup ===\n",
        "SECRET_KEY = os.getenv(\"SECRET_KEY\", config[\"security\"][\"secret_key\"])\n",
        "ALGORITHM = config[\"security\"][\"algorithm\"]\n",
        "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n",
        "\n",
        "def create_access_token(data: dict):\n",
        "    to_encode = data.copy()\n",
        "    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n",
        "    return encoded_jwt\n",
        "\n",
        "def authenticate_user(token: str = Depends(oauth2_scheme)):\n",
        "    try:\n",
        "        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
        "    except jwt.PyJWTError:\n",
        "        logger.warning(\"Authentication failed.\")\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid token\")\n",
        "    return payload\n",
        "\n",
        "# === Pydantic Models ===\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "class TextResponse(BaseModel):\n",
        "    response: str\n",
        "\n",
        "# === NLP Module (T5 Transformer) ===\n",
        "class NLPModule:\n",
        "    def __init__(self):\n",
        "        model_name = config[\"nlp_model\"][\"model_name\"]\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        logger.info(\"NLP model loaded successfully.\")\n",
        "\n",
        "    def generate_text(self, prompt: str) -> str:\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"Prompt cannot be empty.\")\n",
        "        logger.debug(f\"Generating text for prompt: {prompt}\")\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(inputs[\"input_ids\"], max_length=100)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        logger.info(f\"Generated response: {response}\")\n",
        "        return response\n",
        "\n",
        "# === CV Module (YOLOv8 for Object Detection) ===\n",
        "class CVModule:\n",
        "    def __init__(self):\n",
        "        model_path = config[\"cv_model\"][\"model_path\"]\n",
        "        self.model = YOLO(model_path).to(device)\n",
        "        logger.info(\"CV model loaded successfully.\")\n",
        "\n",
        "    def detect_objects(self, image: Image.Image) -> str:\n",
        "        logger.debug(\"Detecting objects in the image.\")\n",
        "        results = self.model(image)\n",
        "        return results.pandas().xyxy[0].to_json()\n",
        "\n",
        "# === Speech Processor (Whisper for Speech-to-Text, PyTTSX3 for Text-to-Speech) ===\n",
        "class SpeechProcessor:\n",
        "    def __init__(self):\n",
        "        whisper_model = config[\"speech_processor\"][\"whisper_model\"]\n",
        "        self.whisper_model = whisper.load_model(whisper_model)\n",
        "        self.tts = pyttsx3.init()\n",
        "        logger.info(\"Speech processor initialized successfully.\")\n",
        "\n",
        "    def speech_to_text(self, audio_file: UploadFile) -> str:\n",
        "        with audio_file.file as audio_data:\n",
        "            result = self.whisper_model.transcribe(audio_data)\n",
        "            return result['text']\n",
        "\n",
        "    def text_to_speech(self, text: str) -> None:\n",
        "        if not text.strip():\n",
        "            raise ValueError(\"Text cannot be empty.\")\n",
        "        self.tts.say(text)\n",
        "        self.tts.runAndWait()\n",
        "\n",
        "    def __del__(self):\n",
        "        self.tts.stop()\n",
        "\n",
        "# === Enhanced AGI Pipeline ===\n",
        "class EnhancedAGIPipeline:\n",
        "    def __init__(self):\n",
        "        self.nlp = NLPModule()\n",
        "        self.cv = CVModule()\n",
        "        self.speech_processor = SpeechProcessor()\n",
        "\n",
        "    async def process_nlp(self, text: str) -> str:\n",
        "        return await asyncio.to_thread(self.nlp.generate_text, text)\n",
        "\n",
        "    async def process_cv(self, image: Image.Image) -> str:\n",
        "        return await asyncio.to_thread(self.cv.detect_objects, image)\n",
        "\n",
        "    async def process_speech_to_text(self, audio_file: UploadFile) -> str:\n",
        "        return await asyncio.to_thread(self.speech_processor.speech_to_text, audio_file)\n",
        "\n",
        "    async def process_text_to_speech(self, text: str) -> None:\n",
        "        await asyncio.to_thread(self.speech_processor.text_to_speech, text)\n",
        "\n",
        "# === FastAPI Application ===\n",
        "app = FastAPI()\n",
        "\n",
        "pipeline = EnhancedAGIPipeline()\n",
        "\n",
        "# === Graceful Shutdown ===\n",
        "def shutdown_signal_handler(sig, frame):\n",
        "    print('Shutting down gracefully...')\n",
        "    sys.exit(0)\n",
        "\n",
        "signal.signal(signal.SIGINT, shutdown_signal_handler)\n",
        "signal.signal(signal.SIGTERM, shutdown_signal_handler)\n",
        "\n",
        "# === Endpoints ===\n",
        "@app.post(\"/process-nlp/\", response_model=TextResponse, dependencies=[Depends(authenticate_user)])\n",
        "async def process_nlp(request: TextRequest):\n",
        "    response = await pipeline.process_nlp(request.text)\n",
        "    return {\"response\": response}\n",
        "\n",
        "@app.post(\"/process-cv-detection/\", dependencies=[Depends(authenticate_user)])\n",
        "async def process_cv_detection(file: UploadFile):\n",
        "    image = Image.open(io.BytesIO(await file.read()))\n",
        "    response = await pipeline.process_cv(image)\n",
        "    return {\"detections\": response}\n",
        "\n",
        "@app.post(\"/batch-cv-detection/\", dependencies=[Depends(authenticate_user)])\n",
        "async def batch_cv_detection(files: List[UploadFile]):\n",
        "    tasks = [pipeline.process_cv(Image.open(io.BytesIO(await file.read()))) for file in files]\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "    return {\"batch_detections\": responses}\n",
        "\n",
        "@app.post(\"/speech-to-text/\", response_model=TextResponse, dependencies=[Depends(authenticate_user)])\n",
        "async def speech_to_text(file: UploadFile):\n",
        "    response = await pipeline.process_speech_to_text(file)\n",
        "    return {\"response\": response}\n",
        "\n",
        "@app.post(\"/text-to-speech/\", dependencies=[Depends(authenticate_user)])\n",
        "async def text_to_speech(request: TextRequest):\n",
        "    await pipeline.process_text_to_speech(request.text)\n",
        "    return {\"response\": \"Speech synthesis complete.\"}\n",
        "\n",
        "# === Run the Application ===\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "UgUAMujBWqGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}