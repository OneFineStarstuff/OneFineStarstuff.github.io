{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPoHH519BuqGSnR/HON75UP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff.github.io/blob/main/agi_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import os\n",
        "import asyncio\n",
        "import time\n",
        "from typing import List\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from PIL import Image\n",
        "from fastapi import FastAPI, UploadFile, Depends, HTTPException, Request\n",
        "from fastapi.security import OAuth2PasswordBearer\n",
        "from pydantic import BaseModel, SecretStr\n",
        "import whisper\n",
        "from ultralytics import YOLO\n",
        "import pyttsx3\n",
        "from loguru import logger\n",
        "import io\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "# === Logging Setup ===\n",
        "logger.add(\"pipeline_{time}.log\", rotation=\"1 MB\", level=\"DEBUG\", enqueue=True, backtrace=True, diagnose=True)\n",
        "logger.info(\"Application startup\")\n",
        "\n",
        "# === Security Enhancement: Environment Variable for Secure Token ===\n",
        "SECURE_TOKEN = SecretStr(os.getenv(\"SECURE_TOKEN\", \"YvZz9Hni0hWJPh_UWW4dQYf9rhIe9nNYcC5ZQTTZz0Q\"))\n",
        "\n",
        "# === OAuth2PasswordBearer for Authentication ===\n",
        "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n",
        "\n",
        "# === Authentication Function ===\n",
        "def authenticate_user(token: str = Depends(oauth2_scheme)):\n",
        "    if token != SECURE_TOKEN.get_secret_value():\n",
        "        logger.warning(\"Authentication failed.\")\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid token\")\n",
        "\n",
        "# === Request and Response Models (Pydantic) ===\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "class TextResponse(BaseModel):\n",
        "    response: str\n",
        "\n",
        "# === NLP Module (T5 Transformer) ===\n",
        "class NLPModule:\n",
        "    def __init__(self):\n",
        "        model_name = \"google/flan-t5-small\"\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        logger.info(\"NLP model loaded successfully.\")\n",
        "\n",
        "    def generate_text(self, prompt: str) -> str:\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"Prompt cannot be empty.\")\n",
        "        logger.debug(f\"Generating text for prompt: {prompt}\")\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        outputs = self.model.generate(inputs[\"input_ids\"], max_length=100)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        logger.info(f\"Generated response: {response}\")\n",
        "        return response\n",
        "\n",
        "# === CV Module (YOLOv8 for Object Detection) ===\n",
        "class CVModule:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = YOLO('yolov8n.pt').to(self.device)\n",
        "        logger.info(\"CV model loaded successfully.\")\n",
        "\n",
        "    def detect_objects(self, image: Image.Image) -> str:\n",
        "        logger.debug(\"Detecting objects in the image.\")\n",
        "        results = self.model(image)\n",
        "        return results.pandas().xyxy[0].to_json()\n",
        "\n",
        "# === Speech Processor (Whisper for Speech-to-Text, PyTTSX3 for Text-to-Speech) ===\n",
        "class SpeechProcessor:\n",
        "    def __init__(self):\n",
        "        self.whisper_model = whisper.load_model(\"base\")\n",
        "        self.tts = pyttsx3.init()\n",
        "        logger.info(\"Speech processor initialized successfully.\")\n",
        "\n",
        "    def speech_to_text(self, audio_file: UploadFile) -> str:\n",
        "        with audio_file.file as audio_data:\n",
        "            result = self.whisper_model.transcribe(audio_data)\n",
        "            return result['text']\n",
        "\n",
        "    def text_to_speech(self, text: str) -> None:\n",
        "        if not text.strip():\n",
        "            raise ValueError(\"Text cannot be empty.\")\n",
        "        self.tts.say(text)\n",
        "        self.tts.runAndWait()\n",
        "\n",
        "    def __del__(self):\n",
        "        self.tts.stop()\n",
        "\n",
        "# === Enhanced AGI Pipeline ===\n",
        "class EnhancedAGIPipeline:\n",
        "    def __init__(self):\n",
        "        self.nlp = NLPModule()\n",
        "        self.cv = CVModule()\n",
        "        self.speech_processor = SpeechProcessor()\n",
        "\n",
        "    async def process_nlp(self, text: str) -> str:\n",
        "        return await asyncio.to_thread(self.nlp.generate_text, text)\n",
        "\n",
        "    async def process_cv(self, image: Image.Image) -> str:\n",
        "        return await asyncio.to_thread(self.cv.detect_objects, image)\n",
        "\n",
        "    async def process_speech_to_text(self, audio_file: UploadFile) -> str:\n",
        "        return await asyncio.to_thread(self.speech_processor.speech_to_text, audio_file)\n",
        "\n",
        "    async def process_text_to_speech(self, text: str) -> None:\n",
        "        await asyncio.to_thread(self.speech_processor.text_to_speech, text)\n",
        "\n",
        "# === FastAPI Application ===\n",
        "app = FastAPI()\n",
        "\n",
        "pipeline = EnhancedAGIPipeline()\n",
        "\n",
        "# === Endpoints ===\n",
        "@app.post(\"/process-nlp/\", response_model=TextResponse, dependencies=[Depends(authenticate_user)])\n",
        "async def process_nlp(request: TextRequest):\n",
        "    response = await pipeline.process_nlp(request.text)\n",
        "    return {\"response\": response}\n",
        "\n",
        "@app.post(\"/process-cv-detection/\", dependencies=[Depends(authenticate_user)])\n",
        "async def process_cv_detection(file: UploadFile):\n",
        "    image = Image.open(io.BytesIO(await file.read()))\n",
        "    response = await pipeline.process_cv(image)\n",
        "    return {\"detections\": response}\n",
        "\n",
        "@app.post(\"/batch-cv-detection/\", dependencies=[Depends(authenticate_user)])\n",
        "async def batch_cv_detection(files: List[UploadFile]):\n",
        "    responses = []\n",
        "    for file in files:\n",
        "        image = Image.open(io.BytesIO(await file.read()))\n",
        "        response = await pipeline.process_cv(image)\n",
        "        responses.append(response)\n",
        "    return {\"batch_detections\": responses}\n",
        "\n",
        "@app.post(\"/speech-to-text/\", response_model=TextResponse, dependencies=[Depends(authenticate_user)])\n",
        "async def speech_to_text(file: UploadFile):\n",
        "    response = await pipeline.process_speech_to_text(file)\n",
        "    return {\"response\": response}\n",
        "\n",
        "@app.post(\"/text-to-speech/\", dependencies=[Depends(authenticate_user)])\n",
        "async def text_to_speech(request: TextRequest):\n",
        "    await pipeline.process_text_to_speech(request.text)\n",
        "    return {\"response\": \"Speech synthesis complete.\"}\n",
        "\n",
        "# === Run the Application with HTTPS (uvicorn) ===\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
        "    server = uvicorn.Server(config)\n",
        "    asyncio.run(server.serve())"
      ],
      "metadata": {
        "id": "UgUAMujBWqGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}